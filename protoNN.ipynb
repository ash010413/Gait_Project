{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"protoNN_example.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"kwwtn5iEQJ1M","colab_type":"text"},"source":["# ProtoNN in Tensorflow\n","\n","This is a simple notebook that illustrates the usage of Tensorflow implementation of ProtoNN. We are using the USPS dataset. Please refer to `fetch_usps.py` for more details on downloading the dataset."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-08-15T13:06:10.223951Z","start_time":"2018-08-15T13:06:09.303454Z"},"id":"dJBVr2b7QJ1R","colab_type":"code","colab":{}},"source":["# Copyright (c) Microsoft Corporation. All rights reserved.\n","# Licensed under the MIT license.\n","\n","from __future__ import print_function\n","import sys\n","import os\n","import numpy as np\n","import tensorflow as tf\n","\n","sys.path.insert(0, '../../')\n","# from edgeml.trainer.protoNNTrainer import ProtoNNTrainer\n","# from edgeml.graph.protoNN import ProtoNN\n","# import edgeml.utils as utils\n","# import helpermethods as helper\n","import pandas as pd\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mxqvfwWQtQ-s","colab_type":"text"},"source":["# Helper Methods"]},{"cell_type":"code","metadata":{"id":"cT-KokQSQiS6","colab_type":"code","colab":{}},"source":["#helper methods\n","sys.path.insert(0, '../')\n","import argparse\n","\n","\n","def getModelSize(matrixList, sparcityList, expected=True, bytesPerVar=4):\n","    '''\n","    expected: Expected size according to the parameters set. The number of\n","        zeros could actually be more than that is required to satisfy the\n","        sparsity constraint.\n","    '''\n","    nnzList, sizeList, isSparseList = [], [], []\n","    hasSparse = False\n","    for i in range(len(matrixList)):\n","        A, s = matrixList[i], sparcityList[i]\n","        assert A.ndim == 2\n","        assert s >= 0\n","        assert s <= 1\n","        nnz, size, sparse = countnnZ(A, s, bytesPerVar=bytesPerVar)\n","        nnzList.append(nnz)\n","        sizeList.append(size)\n","        hasSparse = (hasSparse or sparse)\n","\n","    totalnnZ = np.sum(nnzList)\n","    totalSize = np.sum(sizeList)\n","    if expected:\n","        return totalnnZ, totalSize, hasSparse\n","    numNonZero = 0\n","    totalSize = 0\n","    hasSparse = False\n","    for i in range(len(matrixList)):\n","        A, s = matrixList[i], sparcityList[i]\n","        numNonZero_ = np.count_nonzero(A)\n","        numNonZero += numNonZero_\n","        hasSparse = (hasSparse or (s < 0.5))\n","        if s <= 0.5:\n","            totalSize += numNonZero_ * 2 * bytesPerVar\n","        else:\n","            totalSize += A.size * bytesPerVar\n","    return numNonZero, totalSize, hasSparse\n","\n","\n","def getGamma(gammaInit, projectionDim, dataDim, numPrototypes, x_train):\n","    if gammaInit is None:\n","        print(\"Using median heuristic to estimate gamma.\")\n","        gamma, W, B = medianHeuristic(x_train, projectionDim,\n","                                            numPrototypes)\n","        print(\"Gamma estimate is: %f\" % gamma)\n","        return W, B, gamma\n","    return None, None, gammaInit\n","\n","\n","def preprocessData(dataDir,w):\n","    '''\n","    Loads data from the dataDir and does some initial preprocessing\n","    steps. Data is assumed to be contained in two files,\n","    train.npy and test.npy. Each containing a 2D numpy array of dimension\n","    [numberOfExamples, numberOfFeatures + 1]. The first column of each\n","    matrix is assumed to contain label information.\n","\n","    For an N-Class problem, we assume the labels are integers from 0 through\n","    N-1.\n","    '''\n","    # Uncomment for usual training data\n","    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n","    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n","    # Uncomment for time domain training data\n","    train = np.load(dataDir + '/ttrain_'+str(w)+'.npy')\n","    test = np.load(dataDir + '/ttest_'+str(w)+'.npy')\n","    # Uncomment for 1 sensordrop training data\n","    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n","    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n","\n","    dataDimension = int(train.shape[1]) - 1\n","    x_train = train[:, 1:dataDimension + 1]\n","    y_train_ = train[:, 0]\n","    x_test = test[:, 1:dataDimension + 1]\n","    y_test_ = test[:, 0]\n","\n","    numClasses = max(y_train_) - min(y_train_) + 1\n","    numClasses = max(numClasses, max(y_test_) - min(y_test_) + 1)\n","    numClasses = int(numClasses)\n","\n","    # mean-var\n","    mean = np.mean(x_train, 0)\n","    std = np.std(x_train, 0)\n","    std[std[:] < 0.000001] = 1\n","    x_train = (x_train - mean) / std\n","    x_test = (x_test - mean) / std\n","\n","    # one hot y-train\n","    lab = y_train_.astype('uint8')\n","    lab = np.array(lab) - min(lab)\n","    lab_ = np.zeros((x_train.shape[0], numClasses))\n","    lab_[np.arange(x_train.shape[0]), lab] = 1\n","    y_train = lab_\n","\n","    # one hot y-test\n","    lab = y_test_.astype('uint8')\n","    lab = np.array(lab) - min(lab)\n","    lab_ = np.zeros((x_test.shape[0], numClasses))\n","    lab_[np.arange(x_test.shape[0]), lab] = 1\n","    y_test = lab_\n","\n","    return dataDimension, numClasses, x_train, y_train, x_test, y_test\n","\n","\n","\n","def getProtoNNArgs():\n","    def checkIntPos(value):\n","        ivalue = int(value)\n","        if ivalue <= 0:\n","            raise argparse.ArgumentTypeError(\n","                \"%s is an invalid positive int value\" % value)\n","        return ivalue\n","\n","    def checkIntNneg(value):\n","        ivalue = int(value)\n","        if ivalue < 0:\n","            raise argparse.ArgumentTypeError(\n","                \"%s is an invalid non-neg int value\" % value)\n","        return ivalue\n","\n","    def checkFloatNneg(value):\n","        fvalue = float(value)\n","        if fvalue < 0:\n","            raise argparse.ArgumentTypeError(\n","                \"%s is an invalid non-neg float value\" % value)\n","        return fvalue\n","\n","    def checkFloatPos(value):\n","        fvalue = float(value)\n","        if fvalue <= 0:\n","            raise argparse.ArgumentTypeError(\n","                \"%s is an invalid positive float value\" % value)\n","        return fvalue\n","\n","    '''\n","    Parse protoNN commandline arguments\n","    '''\n","    parser = argparse.ArgumentParser(\n","        description='Hyperparameters for ProtoNN Algorithm')\n","\n","    msg = 'Data directory containing train and test data. The '\n","    msg += 'data is assumed to be saved as 2-D numpy matrices with '\n","    msg += 'names `train.npy` and `test.npy`, of dimensions\\n'\n","    msg += '\\t[numberOfInstances, numberOfFeatures + 1].\\n'\n","    msg += 'The first column of each file is assumed to contain label information.'\n","    msg += ' For a N-class problem, labels are assumed to be integers from 0 to'\n","    msg += ' N-1 (inclusive).'\n","    parser.add_argument('-d', '--data-dir', required=True, help=msg)\n","    parser.add_argument('-l', '--projection-dim', type=checkIntPos, default=10,\n","                        help='Projection Dimension.')\n","    parser.add_argument('-p', '--num-prototypes', type=checkIntPos, default=20,\n","                        help='Number of prototypes.')\n","    parser.add_argument('-g', '--gamma', type=checkFloatPos, default=None,\n","                        help='Gamma for Gaussian kernel. If not provided, ' +\n","                        'median heuristic will be used to estimate gamma.')\n","\n","    parser.add_argument('-e', '--epochs', type=checkIntPos, default=100,\n","                        help='Total training epochs.')\n","    parser.add_argument('-b', '--batch-size', type=checkIntPos, default=32,\n","                        help='Batch size for each pass.')\n","    parser.add_argument('-r', '--learning-rate', type=checkFloatPos,\n","                        default=0.001,\n","                        help='Initial Learning rate for ADAM Optimizer.')\n","\n","    parser.add_argument('-rW', type=float, default=0.000,\n","                        help='Coefficient for l2 regularizer for predictor' +\n","                        ' parameter W ' + '(default = 0.0).')\n","    parser.add_argument('-rB', type=float, default=0.00,\n","                        help='Coefficient for l2 regularizer for predictor' +\n","                        ' parameter B ' + '(default = 0.0).')\n","    parser.add_argument('-rZ', type=float, default=0.00,\n","                        help='Coefficient for l2 regularizer for predictor' +\n","                        'parameter Z ' +\n","                        '(default = 0.0).')\n","\n","    parser.add_argument('-sW', type=float, default=1.000,\n","                        help='Sparsity constraint for predictor parameter W ' +\n","                        '(default = 1.0, i.e. dense matrix).')\n","    parser.add_argument('-sB', type=float, default=1.00,\n","                        help='Sparsity constraint for predictor parameter B ' +\n","                        '(default = 1.0, i.e. dense matrix).')\n","    parser.add_argument('-sZ', type=float, default=1.00,\n","                        help='Sparsity constraint for predictor parameter Z ' +\n","                        '(default = 1.0, i.e. dense matrix).')\n","    parser.add_argument('-pS', '--print-step', type=int, default=200,\n","                        help='The number of update steps between print ' +\n","                        'calls to console.')\n","    parser.add_argument('-vS', '--val-step', type=int, default=3,\n","                        help='The number of epochs between validation' +\n","                        'performance evaluation')\n","    return parser.parse_args()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ou1MfKhYtMdT","colab_type":"text"},"source":["# Utils "]},{"cell_type":"code","metadata":{"id":"jDVo_0JiRSi9","colab_type":"code","colab":{}},"source":["#utils\n","import scipy.cluster\n","import scipy.spatial\n","import os\n","\n","\n","def medianHeuristic(data, projectionDimension, numPrototypes, W_init=None):\n","    '''\n","    This method can be used to estimate gamma for ProtoNN. An approximation to\n","    median heuristic is used here.\n","    1. First the data is collapsed into the projectionDimension by W_init. If\n","    W_init is not provided, it is initialized from a random normal(0, 1). Hence\n","    data normalization is essential.\n","    2. Prototype are computed by running a  k-means clustering on the projected\n","    data.\n","    3. The median distance is then estimated by calculating median distance\n","    between prototypes and projected data points.\n","\n","    data needs to be [-1, numFeats]\n","    If using this method to initialize gamma, please use the W and B as well.\n","\n","    TODO: Return estimate of Z (prototype labels) based on cluster centroids\n","    andand labels\n","\n","    TODO: Clustering fails due to singularity error if projecting upwards\n","\n","    W [dxd_cap]\n","    B [d_cap, m]\n","    returns gamma, W, B\n","    '''\n","    assert data.ndim == 2\n","    X = data\n","    featDim = data.shape[1]\n","    if projectionDimension > featDim:\n","        print(\"Warning: Projection dimension > feature dimension. Gamma\")\n","        print(\"\\t estimation due to median heuristic could fail.\")\n","        print(\"\\tTo retain the projection dataDimension, provide\")\n","        print(\"\\ta value for gamma.\")\n","\n","    if W_init is None:\n","        W_init = np.random.normal(size=[featDim, projectionDimension])\n","    W = W_init\n","    XW = np.matmul(X, W)\n","    assert XW.shape[1] == projectionDimension\n","    assert XW.shape[0] == len(X)\n","    # Requires [N x d_cap] data matrix of N observations of d_cap-dimension and\n","    # the number of centroids m. Returns, [n x d_cap] centroids and\n","    # elementwise center information.\n","    B, centers = scipy.cluster.vq.kmeans2(XW, numPrototypes)\n","    # Requires two matrices. Number of observations x dimension of observation\n","    # space. Distances[i,j] is the distance between XW[i] and B[j]\n","    distances = scipy.spatial.distance.cdist(XW, B, metric='euclidean')\n","    distances = np.reshape(distances, [-1])\n","    gamma = np.median(distances)\n","    gamma = 1 / (2.5 * gamma)\n","    return gamma.astype('float32'), W.astype('float32'), B.T.astype('float32')\n","\n","\n","def multiClassHingeLoss(logits, label, batch_th):\n","    '''\n","    MultiClassHingeLoss to match C++ Version - No TF internal version\n","    '''\n","    flatLogits = tf.reshape(logits, [-1, ])\n","    label_ = tf.argmax(label, 1)\n","\n","    correctId = tf.range(0, batch_th) * label.shape[1] + label_\n","    correctLogit = tf.gather(flatLogits, correctId)\n","\n","    maxLabel = tf.argmax(logits, 1)\n","    top2, _ = tf.nn.top_k(logits, k=2, sorted=True)\n","\n","    wrongMaxLogit = tf.where(\n","        tf.equal(maxLabel, label_), top2[:, 1], top2[:, 0])\n","\n","    return tf.reduce_mean(tf.nn.relu(1. + wrongMaxLogit - correctLogit))\n","\n","\n","def crossEntropyLoss(logits, label):\n","    '''\n","    Cross Entropy loss for MultiClass case in joint training for\n","    faster convergence\n","    '''\n","    return tf.reduce_mean(\n","        tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n","                                                   labels=tf.stop_gradient(label)))\n","\n","\n","def mean_absolute_error(logits, label):\n","    '''\n","    Function to compute the mean absolute error.\n","    '''\n","    return tf.reduce_mean(tf.abs(tf.subtract(logits, label)))\n","\n","\n","def hardThreshold(A, s):\n","    '''\n","    Hard thresholding function on Tensor A with sparsity s\n","    '''\n","    A_ = np.copy(A)\n","    A_ = A_.ravel()\n","    if len(A_) > 0:\n","        th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n","        A_[np.abs(A_) < th] = 0.0\n","    A_ = A_.reshape(A.shape)\n","    return A_\n","\n","\n","def copySupport(src, dest):\n","    '''\n","    copy support of src tensor to dest tensor\n","    '''\n","    support = np.nonzero(src)\n","    dest_ = dest\n","    dest = np.zeros(dest_.shape)\n","    dest[support] = dest_[support]\n","    return dest\n","\n","\n","def countnnZ(A, s, bytesPerVar=4):\n","    '''\n","    Returns # of non-zeros and representative size of the tensor\n","    Uses dense for s >= 0.5 - 4 byte\n","    Else uses sparse - 8 byte\n","    '''\n","    params = 1\n","    hasSparse = False\n","    for i in range(0, len(A.shape)):\n","        params *= int(A.shape[i])\n","    if s < 0.5:\n","        nnZ = np.ceil(params * s)\n","        hasSparse = True\n","        return nnZ, nnZ * 2 * bytesPerVar, hasSparse\n","    else:\n","        nnZ = params\n","        return nnZ, nnZ * bytesPerVar, hasSparse\n","\n","\n","def getConfusionMatrix(predicted, target, numClasses):\n","    '''\n","    Returns a confusion matrix for a multiclass classification\n","    problem. `predicted` is a 1-D array of integers representing\n","    the predicted classes and `target` is the target classes.\n","\n","    confusion[i][j]: Number of elements of class j\n","        predicted as class i\n","    Labels are assumed to be in range(0, numClasses)\n","    Use`printFormattedConfusionMatrix` to echo the confusion matrix\n","    in a user friendly form.\n","    '''\n","    assert(predicted.ndim == 1)\n","    assert(target.ndim == 1)\n","    arr = np.zeros([numClasses, numClasses])\n","\n","    for i in range(len(predicted)):\n","        arr[predicted[i]][target[i]] += 1\n","    return arr\n","\n","\n","def printFormattedConfusionMatrix(matrix):\n","    '''\n","    Given a 2D confusion matrix, prints it in a human readable way.\n","    The confusion matrix is expected to be a 2D numpy array with\n","    square dimensions\n","    '''\n","    assert(matrix.ndim == 2)\n","    assert(matrix.shape[0] == matrix.shape[1])\n","    RECALL = 'Recall'\n","    PRECISION = 'PRECISION'\n","    print(\"|%s|\" % ('True->'), end='')\n","    for i in range(matrix.shape[0]):\n","        print(\"%7d|\" % i, end='')\n","    print(\"%s|\" % 'Precision')\n","\n","    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n","    for i in range(matrix.shape[0]):\n","        print(\"%s|\" % ('-' * 7), end='')\n","    print(\"%s|\" % ('-' * len(PRECISION)))\n","\n","    precisionlist = np.sum(matrix, axis=1)\n","    recalllist = np.sum(matrix, axis=0)\n","    precisionlist = [matrix[i][i] / x if x !=\n","                     0 else -1 for i, x in enumerate(precisionlist)]\n","    recalllist = [matrix[i][i] / x if x !=\n","                  0 else -1 for i, x in enumerate(recalllist)]\n","    for i in range(matrix.shape[0]):\n","        # len recall = 6\n","        print(\"|%6d|\" % (i), end='')\n","        for j in range(matrix.shape[0]):\n","            print(\"%7d|\" % (matrix[i][j]), end='')\n","        print(\"%s\" % (\" \" * (len(PRECISION) - 7)), end='')\n","        if precisionlist[i] != -1:\n","            print(\"%1.5f|\" % precisionlist[i])\n","        else:\n","            print(\"%7s|\" % \"nan\")\n","\n","    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n","    for i in range(matrix.shape[0]):\n","        print(\"%s|\" % ('-' * 7), end='')\n","    print(\"%s|\" % ('-' * len(PRECISION)))\n","    print(\"|%s|\" % ('Recall'), end='')\n","\n","    for i in range(matrix.shape[0]):\n","        if recalllist[i] != -1:\n","            print(\"%1.5f|\" % (recalllist[i]), end='')\n","        else:\n","            print(\"%7s|\" % \"nan\", end='')\n","\n","    print('%s|' % (' ' * len(PRECISION)))\n","\n","\n","def getPrecisionRecall(cmatrix, label=1):\n","    trueP = cmatrix[label][label]\n","    denom = np.sum(cmatrix, axis=0)[label]\n","    if denom == 0:\n","        denom = 1\n","    recall = trueP / denom\n","    denom = np.sum(cmatrix, axis=1)[label]\n","    if denom == 0:\n","        denom = 1\n","    precision = trueP / denom\n","    return precision, recall\n","\n","\n","def getMacroPrecisionRecall(cmatrix):\n","    # TP + FP\n","    precisionlist = np.sum(cmatrix, axis=1)\n","    # TP + FN\n","    recalllist = np.sum(cmatrix, axis=0)\n","    precisionlist__ = [cmatrix[i][i] / x if x !=\n","                       0 else 0 for i, x in enumerate(precisionlist)]\n","    recalllist__ = [cmatrix[i][i] / x if x !=\n","                    0 else 0 for i, x in enumerate(recalllist)]\n","    precision = np.sum(precisionlist__)\n","    precision /= len(precisionlist__)\n","    recall = np.sum(recalllist__)\n","    recall /= len(recalllist__)\n","    return precision, recall\n","\n","\n","def getMicroPrecisionRecall(cmatrix):\n","    # TP + FP\n","    precisionlist = np.sum(cmatrix, axis=1)\n","    # TP + FN\n","    recalllist = np.sum(cmatrix, axis=0)\n","    num = 0.0\n","    for i in range(len(cmatrix)):\n","        num += cmatrix[i][i]\n","\n","    precision = num / np.sum(precisionlist)\n","    recall = num / np.sum(recalllist)\n","    return precision, recall\n","\n","\n","def getMacroMicroFScore(cmatrix):\n","    '''\n","    Returns macro and micro f-scores.\n","    Refer: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n","    '''\n","    precisionlist = np.sum(cmatrix, axis=1)\n","    recalllist = np.sum(cmatrix, axis=0)\n","    precisionlist__ = [cmatrix[i][i] / x if x !=\n","                       0 else 0 for i, x in enumerate(precisionlist)]\n","    recalllist__ = [cmatrix[i][i] / x if x !=\n","                    0 else 0 for i, x in enumerate(recalllist)]\n","    macro = 0.0\n","    for i in range(len(precisionlist)):\n","        denom = precisionlist__[i] + recalllist__[i]\n","        numer = precisionlist__[i] * recalllist__[i] * 2\n","        if denom == 0:\n","            denom = 1\n","        macro += numer / denom\n","    macro /= len(precisionlist)\n","\n","    num = 0.0\n","    for i in range(len(precisionlist)):\n","        num += cmatrix[i][i]\n","\n","    denom1 = np.sum(precisionlist)\n","    denom2 = np.sum(recalllist)\n","    pi = num / denom1\n","    rho = num / denom2\n","    denom = pi + rho\n","    if denom == 0:\n","        denom = 1\n","    micro = 2 * pi * rho / denom\n","    return macro, micro\n","\n","\n","class GraphManager:\n","    '''\n","    Manages saving and restoring graphs. Designed to be used with EMI-RNN\n","    though is general enough to be useful otherwise as well.\n","    '''\n","\n","    def __init__(self):\n","        pass\n","\n","    def checkpointModel(self, saver, sess, modelPrefix,\n","                        globalStep=1000, redirFile=None):\n","        saver.save(sess, modelPrefix, global_step=globalStep)\n","        print('Model saved to %s, global_step %d' % (modelPrefix, globalStep),\n","              file=redirFile)\n","\n","    def loadCheckpoint(self, sess, modelPrefix, globalStep,\n","                       redirFile=None):\n","        metaname = modelPrefix + '-%d.meta' % globalStep\n","        basename = os.path.basename(metaname)\n","        fileList = os.listdir(os.path.dirname(modelPrefix))\n","        fileList = [x for x in fileList if x.startswith(basename)]\n","        assert len(fileList) > 0, 'Checkpoint file not found'\n","        msg = 'Too many or too few checkpoint files for globalStep: %d' % globalStep\n","        assert len(fileList) is 1, msg\n","        chkpt = basename + '/' + fileList[0]\n","        saver = tf.train.import_meta_graph(metaname)\n","        metaname = metaname[:-5]\n","        saver.restore(sess, metaname)\n","        graph = tf.get_default_graph()\n","        return graph"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DAjSVSOFtFmm","colab_type":"text"},"source":["# Model Trainer - ProtoNN"]},{"cell_type":"code","metadata":{"id":"bp5dEFiZR_sy","colab_type":"code","colab":{}},"source":["#Trainer\n","class ProtoNNTrainer:\n","    def __init__(self, protoNNObj, regW, regB, regZ,\n","                 sparcityW, sparcityB, sparcityZ,\n","                 learningRate, X, Y, lossType='l2'):\n","        '''\n","        A wrapper for the various techniques used for training ProtoNN. This\n","        subsumes both the responsibility of loss graph construction and\n","        performing training. The original training routine that is part of the\n","        C++ implementation of EdgeML used iterative hard thresholding (IHT),\n","        gamma estimation through median heuristic and other tricks for\n","        training ProtoNN. This module implements the same in Tensorflow\n","        and python.\n","\n","        protoNNObj: An instance of ProtoNN class defining the forward\n","            computation graph. The loss functions and training routines will be\n","            attached to this instance.\n","        regW, regB, regZ: Regularization constants for W, B, and\n","            Z matrices of protoNN.\n","        sparcityW, sparcityB, sparcityZ: Sparsity constraints\n","            for W, B and Z matrices. A value between 0 (exclusive) and 1\n","            (inclusive) is expected. A value of 1 indicates dense training.\n","        learningRate: Initial learning rate for ADAM optimizer.\n","        X, Y : Placeholders for data and labels.\n","            X [-1, featureDimension]\n","            Y [-1, num Labels]\n","        lossType: ['l2', 'xentropy']\n","        '''\n","        self.protoNNObj = protoNNObj\n","        self.__regW = regW\n","        self.__regB = regB\n","        self.__regZ = regZ\n","        self.__sW = sparcityW\n","        self.__sB = sparcityB\n","        self.__sZ = sparcityZ\n","        self.__lR = learningRate\n","        self.X = X\n","        self.Y = Y\n","        self.sparseTraining = True\n","        if (sparcityW == 1.0) and (sparcityB == 1.0) and (sparcityZ == 1.0):\n","            self.sparseTraining = False\n","            print(\"Sparse training disabled.\", file=sys.stderr)\n","        # Define placeholders for sparse training\n","        self.W_th = None\n","        self.B_th = None\n","        self.Z_th = None\n","        self.__lossType = lossType\n","        self.__validInit = False\n","        self.__validInit = self.__validateInit()\n","        self.__protoNNOut = protoNNObj(X, Y)\n","        self.loss = self.__lossGraph()\n","        self.trainStep = self.__trainGraph()\n","        self.__hthOp = self.__getHardThresholdOp()\n","        self.accuracy = protoNNObj.getAccuracyOp()\n","\n","    def __validateInit(self):\n","        self.__validInit = False\n","        msg = \"Sparsity value should be between\"\n","        msg += \" 0 and 1 (both inclusive).\"\n","        assert self.__sW >= 0. and self.__sW <= 1., 'W:' + msg\n","        assert self.__sB >= 0. and self.__sB <= 1., 'B:' + msg\n","        assert self.__sZ >= 0. and self.__sZ <= 1., 'Z:' + msg\n","        d, dcap, m, L, _ = self.protoNNObj.getHyperParams()\n","        msg = 'Y should be of dimension [-1, num labels/classes]'\n","        msg += ' specified as part of ProtoNN object.'\n","        assert (len(self.Y.shape)) == 2, msg\n","        assert (self.Y.shape[1] == L), msg\n","        msg = 'X should be of dimension [-1, featureDimension]'\n","        msg += ' specified as part of ProtoNN object.'\n","        assert (len(self.X.shape) == 2), msg\n","        assert (self.X.shape[1] == d), msg\n","        self.__validInit = True\n","        msg = 'Values can be \\'l2\\', or \\'xentropy\\''\n","        if self.__lossType not in ['l2', 'xentropy']:\n","            raise ValueError(msg)\n","        return True\n","\n","    def __lossGraph(self):\n","        pnnOut = self.__protoNNOut\n","        l1, l2, l3 = self.__regW, self.__regB, self.__regZ\n","        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n","        if self.__lossType == 'l2':\n","            with tf.name_scope('protonn-l2-loss'):\n","                loss_0 = tf.nn.l2_loss(self.Y - pnnOut)\n","                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n","                reg += l3 * tf.nn.l2_loss(Z)\n","                loss = loss_0 + reg\n","        elif self.__lossType == 'xentropy':\n","            with tf.name_scope('protonn-xentropy-loss'):\n","                loss_0 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pnnOut,\n","                                                         labels=tf.stop_gradient(self.Y))\n","                loss_0 = tf.reduce_mean(loss_0)\n","                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n","                reg += l3 * tf.nn.l2_loss(Z)\n","                loss = loss_0 + reg\n","        return loss\n","\n","    def __trainGraph(self):\n","        with tf.name_scope('protonn-gradient-adam'):\n","            trainStep = tf.train.AdamOptimizer(self.__lR)\n","            trainStep = trainStep.minimize(self.loss)\n","        return trainStep\n","\n","    def __getHardThresholdOp(self):\n","        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n","        self.W_th = tf.placeholder(tf.float32, name='W_th')\n","        self.B_th = tf.placeholder(tf.float32, name='B_th')\n","        self.Z_th = tf.placeholder(tf.float32, name='Z_th')\n","        with tf.name_scope('hard-threshold-assignments'):\n","            hard_thrsd_W = W.assign(self.W_th)\n","            hard_thrsd_B = B.assign(self.B_th)\n","            hard_thrsd_Z = Z.assign(self.Z_th)\n","            hard_thrsd_op = tf.group(hard_thrsd_W, hard_thrsd_B, hard_thrsd_Z)\n","        return hard_thrsd_op\n","\n","    def train(self, batchSize, totalEpochs, sess,\n","              x_train, x_val, y_train, y_val, noInit=False,\n","              redirFile=None, printStep=10, valStep=3):\n","        '''\n","        Performs dense training of ProtoNN followed by iterative hard\n","        thresholding to enforce sparsity constraints.\n","\n","        batchSize: Batch size per update\n","        totalEpochs: The number of epochs to run training for. One epoch is\n","            defined as one pass over the entire training data.\n","        sess: The Tensorflow session to use for running various graph\n","            operators.\n","        x_train, x_val, y_train, y_val: The numpy array containing train and\n","            validation data. x data is assumed to in of shape [-1,\n","            featureDimension] while y should have shape [-1, numberLabels].\n","        noInit: By default, all the tensors of the computation graph are\n","        initialized at the start of the training session. Set noInit=False to\n","        disable this behaviour.\n","        printStep: Number of batches between echoing of loss and train accuracy.\n","        valStep: Number of epochs between evolutions on validation set.\n","        '''\n","        d, d_cap, m, L, gamma = self.protoNNObj.getHyperParams()\n","        assert batchSize >= 1, 'Batch size should be positive integer'\n","        assert totalEpochs >= 1, 'Total epochs should be positive integer'\n","        assert x_train.ndim == 2, 'Expected training data to be of rank 2'\n","        assert x_train.shape[1] == d, 'Expected x_train to be [-1, %d]' % d\n","        assert x_val.ndim == 2, 'Expected validation data to be of rank 2'\n","        assert x_val.shape[1] == d, 'Expected x_val to be [-1, %d]' % d\n","        assert y_train.ndim == 2, 'Expected training labels to be of rank 2'\n","        assert y_train.shape[1] == L, 'Expected y_train to be [-1, %d]' % L\n","        assert y_val.ndim == 2, 'Expected validation labels to be of rank 2'\n","        assert y_val.shape[1] == L, 'Expected y_val to be [-1, %d]' % L\n","\n","        # Numpy will throw asserts for arrays\n","        if sess is None:\n","            raise ValueError('sess must be valid Tensorflow session.')\n","\n","        trainNumBatches = int(np.ceil(len(x_train) / batchSize))\n","        valNumBatches = int(np.ceil(len(x_val) / batchSize))\n","        x_train_batches = np.array_split(x_train, trainNumBatches)\n","        y_train_batches = np.array_split(y_train, trainNumBatches)\n","        x_val_batches = np.array_split(x_val, valNumBatches)\n","        y_val_batches = np.array_split(y_val, valNumBatches)\n","        if not noInit:\n","            sess.run(tf.global_variables_initializer())\n","        X, Y = self.X, self.Y\n","        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n","        for epoch in range(totalEpochs):\n","            for i in range(len(x_train_batches)):\n","                batch_x = x_train_batches[i]\n","                batch_y = y_train_batches[i]\n","                feed_dict = {\n","                    X: batch_x,\n","                    Y: batch_y\n","                }\n","                sess.run(self.trainStep, feed_dict=feed_dict)\n","                if i % printStep == 0:\n","                    loss, acc = sess.run([self.loss, self.accuracy],\n","                                         feed_dict=feed_dict)\n","                    msg = \"Epoch: %3d Batch: %3d\" % (epoch, i)\n","                    msg += \" Loss: %3.5f Accuracy: %2.5f\" % (loss, acc)\n","                    print(msg, file=redirFile)\n","\n","            # Perform Hard thresholding\n","            if self.sparseTraining:\n","                W_, B_, Z_ = sess.run([W, B, Z])\n","                fd_thrsd = {\n","                    self.W_th: hardThreshold(W_, self.__sW),\n","                    self.B_th: hardThreshold(B_, self.__sB),\n","                    self.Z_th: hardThreshold(Z_, self.__sZ)\n","                }\n","                sess.run(self.__hthOp, feed_dict=fd_thrsd)\n","\n","            if (epoch + 1) % valStep  == 0:\n","                acc = 0.0\n","                loss = 0.0\n","                for j in range(len(x_val_batches)):\n","                    batch_x = x_val_batches[j]\n","                    batch_y = y_val_batches[j]\n","                    feed_dict = {\n","                        X: batch_x,\n","                        Y: batch_y\n","                    }\n","                    acc_, loss_ = sess.run([self.accuracy, self.loss],\n","                                           feed_dict=feed_dict)\n","                    acc += acc_\n","                    loss += loss_\n","                acc /= len(y_val_batches)\n","                loss /= len(y_val_batches)\n","                print(\"Test Loss: %2.5f Accuracy: %2.5f\" % (loss, acc))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Z6ym4k_s9pS","colab_type":"text"},"source":["# Model Graph - ProtoNN"]},{"cell_type":"code","metadata":{"id":"GRPFglKHSbu-","colab_type":"code","colab":{}},"source":["\n","class ProtoNN:\n","    def __init__(self, inputDimension, projectionDimension, numPrototypes,\n","                 numOutputLabels, gamma,\n","                 W = None, B = None, Z = None):\n","        '''\n","        Forward computation graph for ProtoNN.\n","\n","        inputDimension: Input data dimension or feature dimension.\n","        projectionDimension: hyperparameter\n","        numPrototypes: hyperparameter\n","        numOutputLabels: The number of output labels or classes\n","        W, B, Z: Numpy matrices that can be used to initialize\n","            projection matrix(W), prototype matrix (B) and prototype labels\n","            matrix (B).\n","            Expected Dimensions:\n","                W   inputDimension (d) x projectionDimension (d_cap)\n","                B   projectionDimension (d_cap) x numPrototypes (m)\n","                Z   numOutputLabels (L) x numPrototypes (m)\n","        '''\n","        with tf.name_scope('protoNN') as ns:\n","            self.__nscope = ns\n","        self.__d = inputDimension\n","        self.__d_cap = projectionDimension\n","        self.__m = numPrototypes\n","        self.__L = numOutputLabels\n","\n","        self.__inW = W\n","        self.__inB = B\n","        self.__inZ = Z\n","        self.__inGamma = gamma\n","        self.W, self.B, self.Z = None, None, None\n","        self.gamma = None\n","\n","        self.__validInit = False\n","        self.__initWBZ()\n","        self.__initGamma()\n","        self.__validateInit()\n","        self.protoNNOut = None\n","        self.predictions = None\n","        self.accuracy = None\n","\n","    def __validateInit(self):\n","        self.__validInit = False\n","        errmsg = \"Dimensions mismatch! Should be W[d, d_cap]\"\n","        errmsg += \", B[d_cap, m] and Z[L, m]\"\n","        d, d_cap, m, L, _ = self.getHyperParams()\n","        assert self.W.shape[0] == d, errmsg\n","        assert self.W.shape[1] == d_cap, errmsg\n","        assert self.B.shape[0] == d_cap, errmsg\n","        assert self.B.shape[1] == m, errmsg\n","        assert self.Z.shape[0] == L, errmsg\n","        assert self.Z.shape[1] == m, errmsg\n","        self.__validInit = True\n","\n","    def __initWBZ(self):\n","        with tf.name_scope(self.__nscope):\n","            W = self.__inW\n","            if W is None:\n","                W = tf.random_normal_initializer()\n","                W = W([self.__d, self.__d_cap])\n","            self.W = tf.Variable(W, name='W', dtype=tf.float32)\n","\n","            B = self.__inB\n","            if B is None:\n","                B = tf.random_uniform_initializer()\n","                B = B([self.__d_cap, self.__m])\n","            self.B = tf.Variable(B, name='B', dtype=tf.float32)\n","\n","            Z = self.__inZ\n","            if Z is None:\n","                Z = tf.random_normal_initializer()\n","                Z = Z([self.__L, self.__m])\n","            Z = tf.Variable(Z, name='Z', dtype=tf.float32)\n","            self.Z = Z\n","        return self.W, self.B, self.Z\n","\n","    def __initGamma(self):\n","        with tf.name_scope(self.__nscope):\n","            gamma = self.__inGamma\n","            self.gamma = tf.constant(gamma, name='gamma')\n","\n","    def getHyperParams(self):\n","        '''\n","        Returns the model hyperparameters:\n","            [inputDimension, projectionDimension,\n","            numPrototypes, numOutputLabels, gamma]\n","        '''\n","        d = self.__d\n","        dcap = self.__d_cap\n","        m = self.__m\n","        L = self.__L\n","        return d, dcap, m, L, self.gamma\n","\n","    def getModelMatrices(self):\n","        '''\n","        Returns Tensorflow tensors of the model matrices, which\n","        can then be evaluated to obtain corresponding numpy arrays.\n","\n","        These can then be exported as part of other implementations of\n","        ProtonNN, for instance a C++ implementation or pure python\n","        implementation.\n","        Returns\n","            [ProjectionMatrix (W), prototypeMatrix (B),\n","             prototypeLabelsMatrix (Z), gamma]\n","        '''\n","        return self.W, self.B, self.Z, self.gamma\n","\n","    def __call__(self, X, Y=None):\n","        '''\n","        This method is responsible for construction of the forward computation\n","        graph. The end point of the computation graph, or in other words the\n","        output operator for the forward computation is returned. Additionally,\n","        if the argument Y is provided, a classification accuracy operator with\n","        Y as target will also be created. For this, Y is assumed to in one-hot\n","        encoded format and the class with the maximum prediction score is\n","        compared to the encoded class in Y.  This accuracy operator is returned\n","        by getAccuracyOp() method. If a different accuracyOp is required, it\n","        can be defined by overriding the createAccOp(protoNNScoresOut, Y)\n","        method.\n","\n","        X: Input tensor or placeholder of shape [-1, inputDimension]\n","        Y: Optional tensor or placeholder for targets (labels or classes).\n","            Expected shape is [-1, numOutputLabels].\n","        returns: The forward computation outputs, self.protoNNOut\n","        '''\n","        # This should never execute\n","        assert self.__validInit is True, \"Initialization failed!\"\n","        if self.protoNNOut is not None:\n","            return self.protoNNOut\n","\n","        W, B, Z, gamma = self.W, self.B, self.Z, self.gamma\n","        with tf.name_scope(self.__nscope):\n","            WX = tf.matmul(X, W)\n","            # Convert WX to tensor so that broadcasting can work\n","            dim = [-1, WX.shape.as_list()[1], 1]\n","            WX = tf.reshape(WX, dim)\n","            dim = [1, B.shape.as_list()[0], -1]\n","            B = tf.reshape(B, dim)\n","            l2sim = B - WX\n","            l2sim = tf.pow(l2sim, 2)\n","            l2sim = tf.reduce_sum(l2sim, 1, keepdims=True)\n","            self.l2sim = l2sim\n","            gammal2sim = (-1 * gamma * gamma) * l2sim\n","            M = tf.exp(gammal2sim)\n","            dim = [1] + Z.shape.as_list()\n","            Z = tf.reshape(Z, dim)\n","            y = tf.multiply(Z, M)\n","            y = tf.reduce_sum(y, 2, name='protoNNScoreOut')\n","            self.protoNNOut = y\n","            self.predictions = tf.argmax(y, 1, name='protoNNPredictions')\n","            if Y is not None:\n","                self.createAccOp(self.protoNNOut, Y)\n","        return y\n","\n","    def createAccOp(self, outputs, target):\n","        '''\n","        Define an accuracy operation on ProtoNN's output scores and targets.\n","        Here a simple classification accuracy operator is defined. More\n","        complicated operators (for multiple label problems and so forth) can be\n","        defined by overriding this method\n","        '''\n","        assert self.predictions is not None\n","        target = tf.argmax(target, 1)\n","        correctPrediction = tf.equal(self.predictions, target)\n","        acc = tf.reduce_mean(tf.cast(correctPrediction, tf.float32),\n","                             name='protoNNAccuracy')\n","        self.accuracy = acc\n","\n","    def getPredictionsOp(self):\n","        '''\n","        The predictions operator is defined as argmax(protoNNScores) for each\n","        prediction.\n","        '''\n","        return self.predictions\n","\n","    def getAccuracyOp(self):\n","        '''\n","        returns accuracyOp as defined by createAccOp. It defaults to\n","        multi-class classification accuracy.\n","        '''\n","        msg = \"Accuracy operator not defined in graph. Did you provide Y as an\"\n","        msg += \" argument to _call_?\"\n","        assert self.accuracy is not None, msg\n","        return self.accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uEBMKewPQJ1c","colab_type":"text"},"source":["# Obtain Data\n","\n","It is assumed that the Daphnet data has already been downloaded,preprocessed and set up in subdirectory."]},{"cell_type":"code","metadata":{"id":"ZvR1S9WxRRo_","colab_type":"code","outputId":"fb3a66b1-5737-4070-9d7d-f717ec1dc067","executionInfo":{"status":"ok","timestamp":1567153592001,"user_tz":-330,"elapsed":4532,"user":{"displayName":"Gokul Hari","photoUrl":"","userId":"16159457985484250305"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive \n","drive.mount (\"/content/gdrive\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZTTXAaKaTh4_","colab_type":"code","outputId":"78814999-01d5-46bb-f061-3f51a6d57a64","executionInfo":{"status":"ok","timestamp":1567150762680,"user_tz":-330,"elapsed":899,"user":{"displayName":"Gokul Hari","photoUrl":"","userId":"16159457985484250305"}},"colab":{"base_uri":"https://localhost:8080/","height":144}},"source":["# # Run only to create ProtoNN files\n","\n","# os.chdir(\"/content/gdrive/My Drive/Recovery/dataset_fog_release/dataset/features\")\n","# windowLen = 2\n","# tdf = pd.read_csv(\"time_\"+ str(windowLen)+\".csv\")\n","# fdf = pd.read_csv(\"freq_\"+ str(windowLen)+\".csv\")\n","# df = pd.concat([tdf,fdf],axis = 1)\n","\n","# print (df['0'].value_counts())\n","\n","# df = df[df['0'] != 2]\n","# print (df['0'].value_counts())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0    11365\n","1     1650\n","2      365\n","Name: 0, dtype: int64\n","0    11365\n","1     1650\n","Name: 0, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WZcXf9sd-yze","colab_type":"code","outputId":"1e06c98d-28dc-4804-b228-f8cfff16967d","executionInfo":{"status":"ok","timestamp":1567150927992,"user_tz":-330,"elapsed":1111,"user":{"displayName":"Gokul Hari","photoUrl":"","userId":"16159457985484250305"}},"colab":{"base_uri":"https://localhost:8080/","height":290}},"source":["#single sensor test\n","# s=df.columns\n","# sensor = [\"A_\",\"T_\",\"L_\"]\n","# count=0\n","\n","# for i in (sensor):\n","#   print(\"Dropping sensor:%s , count is %d \", i,count)\n","#   temp=[]\n","#   for j in (s):\n","#     if((j.find(i)<0)&(j!='0')): # (< : choose unmatched, for single sensor test) (>  : choose matched, two sensor test ) \n","#       temp.append(j)\n","#   if (count==0):\n","#     s1=temp\n","#     print(len(s1))\n","#   if (count==1):\n","#     s2=temp\n","#     print(len(s2))\n","#   if (count==2):\n","#     s3=temp\n","#     print(len(s3))\n","#   count+=1\n","\n","# s1=list(s1)\n","# s2=list(s2)\n","# s3=list(s3)\n","# d = [s1,s2,s3]\n","# sensor = [\"A\",\"T\",\"L\"] # files with \"test_A_\" are two sensor df, containing remainimg two sensors( i.e, _A_ mean T and L are present). files with \"test_A\" are single sensor df\n","# DATA_DIR = \"/content/gdrive/My Drive/Recovery/dataset_fog_release/dataset/DATA_DIR\"\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.model_selection import cross_val_score\n","# from imblearn.over_sampling import SMOTE\n","\n","# for i in range(0,3):\n","#   X = df.drop(columns=(d[i]))\n","#   y = df['0']\n","#   print(df.shape,X.shape)\n","  \n","#   X_resampled, y_resampled = SMOTE().fit_resample(X,y)\n","#   print (X_resampled.shape,y_resampled.shape)\n","#   train,test,_,_ = train_test_split(X_resampled,y_resampled, train_size = 0.7, stratify = y_resampled)\n","#   print (train.shape,test.shape)\n","#   ttrain = DATA_DIR + \"/train_\"+sensor[i]\n","#   ttest = DATA_DIR + \"/test_\"+sensor[i]\n","#   np.save(ttrain,train)\n","#   np.save(ttest,test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Dropping sensor:%s , count is %d  A_ 0\n","60\n","Dropping sensor:%s , count is %d  T_ 1\n","60\n","Dropping sensor:%s , count is %d  L_ 2\n","60\n","(13015, 91) (13015, 31)\n","(22730, 31) (22730,)\n","(15910, 31) (6820, 31)\n","(13015, 91) (13015, 31)\n","(22730, 31) (22730,)\n","(15910, 31) (6820, 31)\n","(13015, 91) (13015, 31)\n","(22730, 31) (22730,)\n","(15910, 31) (6820, 31)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g0degz-XXwzM","colab_type":"code","outputId":"46589cce-85e4-4b30-f8d9-34d6e0be13e5","executionInfo":{"status":"ok","timestamp":1567154485307,"user_tz":-330,"elapsed":2176,"user":{"displayName":"Gokul Hari","photoUrl":"","userId":"16159457985484250305"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["DATA_DIR = \"/content/gdrive/My Drive/Recovery/dataset_fog_release/dataset/DATA_DIR\"\n","windowLen = '4'\n","out = preprocessData(DATA_DIR,windowLen)\n","dataDimension = out[0]\n","numClasses = out[1]\n","x_train, y_train = out[2], out[3]\n","x_test, y_test = out[4], out[5]\n","print(\"Feature Dimension: \", dataDimension)\n","print(\"Num classes: \", numClasses)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Feature Dimension:  45\n","Num classes:  2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1u6oX8eJQJ2N","colab_type":"text"},"source":["# Model Parameters\n","\n","Note that ProtoNN is very sensitive to the value of the hyperparameter $\\gamma$, here stored in valiable `GAMMA`. If `GAMMA` is set to `None`, median heuristic will be used to estimate a good value of $\\gamma$ through the `helper.getGamma()` method. This method also returns the corresponding `W` and `B` matrices which should be used to initialize ProtoNN (as is done here)."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-08-15T13:06:10.279204Z","start_time":"2018-08-15T13:06:10.272880Z"},"id":"UaduZ1vJQJ2P","colab_type":"code","colab":{}},"source":["PROJECTION_DIM = 5 #d^\n","NUM_PROTOTYPES = 40 #m\n","REG_W = 0.000005\n","REG_B = 0.0\n","REG_Z = 0.00005\n","SPAR_W = 1.0\n","SPAR_B = 0.8\n","SPAR_Z = 0.8\n","LEARNING_RATE = 0.05\n","NUM_EPOCHS = 200\n","BATCH_SIZE = 32\n","GAMMA = None"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-08-15T13:06:10.307632Z","start_time":"2018-08-15T13:06:10.280955Z"},"id":"teqlUPhLQJ2W","colab_type":"code","outputId":"e7e7f7f2-9ddb-448b-9539-65a1a2dc1c03","executionInfo":{"status":"ok","timestamp":1567154485603,"user_tz":-330,"elapsed":1003,"user":{"displayName":"Gokul Hari","photoUrl":"","userId":"16159457985484250305"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["W, B, gamma = getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n","                       NUM_PROTOTYPES, x_train)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using median heuristic to estimate gamma.\n","Gamma estimate is: 0.024634\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/cluster/vq.py:579: UserWarning: One of the clusters is empty. Re-run kmeans with a different initialization.\n","  warnings.warn(\"One of the clusters is empty. \"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"BJwO4MXatk9G","colab_type":"text"},"source":["# Model Training"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-08-15T13:07:22.641991Z","start_time":"2018-08-15T13:06:10.309353Z"},"scrolled":true,"id":"MrKAP5_RQJ2b","colab_type":"code","outputId":"2fb982af-47ae-4867-c5e5-b2ecc2b9dfc4","executionInfo":{"status":"ok","timestamp":1567154584840,"user_tz":-330,"elapsed":98358,"user":{"displayName":"Gokul Hari","photoUrl":"","userId":"16159457985484250305"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Setup input and train protoNN\n","X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n","Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n","protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n","                  NUM_PROTOTYPES, numClasses,\n","                  gamma, W=W, B=B)\n","trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n","                         SPAR_W, SPAR_B, SPAR_Z,\n","                         LEARNING_RATE, X, Y, lossType='xentropy')\n","sess = tf.Session()\n","\n","trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,\n","              printStep=600, valStep=10)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch:   0 Batch:   0 Loss: 1.00354 Accuracy: 0.43750\n","Epoch:   1 Batch:   0 Loss: 0.42132 Accuracy: 0.81250\n","Epoch:   2 Batch:   0 Loss: 0.32789 Accuracy: 0.87500\n","Epoch:   3 Batch:   0 Loss: 0.27416 Accuracy: 0.90625\n","Epoch:   4 Batch:   0 Loss: 0.22981 Accuracy: 0.90625\n","Epoch:   5 Batch:   0 Loss: 0.19345 Accuracy: 0.90625\n","Epoch:   6 Batch:   0 Loss: 0.18216 Accuracy: 0.93750\n","Epoch:   7 Batch:   0 Loss: 0.16916 Accuracy: 0.93750\n","Epoch:   8 Batch:   0 Loss: 0.16058 Accuracy: 0.93750\n","Epoch:   9 Batch:   0 Loss: 0.15968 Accuracy: 0.93750\n","Test Loss: 0.18994 Accuracy: 0.93626\n","Epoch:  10 Batch:   0 Loss: 0.16350 Accuracy: 0.93750\n","Epoch:  11 Batch:   0 Loss: 0.16202 Accuracy: 0.93750\n","Epoch:  12 Batch:   0 Loss: 0.15778 Accuracy: 0.93750\n","Epoch:  13 Batch:   0 Loss: 0.15651 Accuracy: 0.93750\n","Epoch:  14 Batch:   0 Loss: 0.15024 Accuracy: 0.93750\n","Epoch:  15 Batch:   0 Loss: 0.15050 Accuracy: 0.93750\n","Epoch:  16 Batch:   0 Loss: 0.14787 Accuracy: 0.93750\n","Epoch:  17 Batch:   0 Loss: 0.14375 Accuracy: 0.93750\n","Epoch:  18 Batch:   0 Loss: 0.14500 Accuracy: 0.93750\n","Epoch:  19 Batch:   0 Loss: 0.14475 Accuracy: 0.93750\n","Test Loss: 0.17505 Accuracy: 0.94934\n","Epoch:  20 Batch:   0 Loss: 0.14397 Accuracy: 0.93750\n","Epoch:  21 Batch:   0 Loss: 0.14347 Accuracy: 0.93750\n","Epoch:  22 Batch:   0 Loss: 0.14181 Accuracy: 0.93750\n","Epoch:  23 Batch:   0 Loss: 0.14350 Accuracy: 0.93750\n","Epoch:  24 Batch:   0 Loss: 0.14212 Accuracy: 0.93750\n","Epoch:  25 Batch:   0 Loss: 0.14035 Accuracy: 0.93750\n","Epoch:  26 Batch:   0 Loss: 0.14016 Accuracy: 0.93750\n","Epoch:  27 Batch:   0 Loss: 0.13907 Accuracy: 0.93750\n","Epoch:  28 Batch:   0 Loss: 0.13456 Accuracy: 0.93750\n","Epoch:  29 Batch:   0 Loss: 0.13256 Accuracy: 0.93750\n","Test Loss: 0.17411 Accuracy: 0.94904\n","Epoch:  30 Batch:   0 Loss: 0.13087 Accuracy: 0.93750\n","Epoch:  31 Batch:   0 Loss: 0.12932 Accuracy: 0.93750\n","Epoch:  32 Batch:   0 Loss: 0.12789 Accuracy: 0.93750\n","Epoch:  33 Batch:   0 Loss: 0.12688 Accuracy: 0.93750\n","Epoch:  34 Batch:   0 Loss: 0.12579 Accuracy: 0.93750\n","Epoch:  35 Batch:   0 Loss: 0.12481 Accuracy: 0.93750\n","Epoch:  36 Batch:   0 Loss: 0.14048 Accuracy: 0.93750\n","Epoch:  37 Batch:   0 Loss: 0.12699 Accuracy: 0.93750\n","Epoch:  38 Batch:   0 Loss: 0.12416 Accuracy: 0.93750\n","Epoch:  39 Batch:   0 Loss: 0.12296 Accuracy: 0.93750\n","Test Loss: 0.17334 Accuracy: 0.94950\n","Epoch:  40 Batch:   0 Loss: 0.12196 Accuracy: 0.93750\n","Epoch:  41 Batch:   0 Loss: 0.12894 Accuracy: 0.93750\n","Epoch:  42 Batch:   0 Loss: 0.12200 Accuracy: 0.93750\n","Epoch:  43 Batch:   0 Loss: 0.12018 Accuracy: 0.93750\n","Epoch:  44 Batch:   0 Loss: 0.11922 Accuracy: 0.93750\n","Epoch:  45 Batch:   0 Loss: 0.12761 Accuracy: 0.93750\n","Epoch:  46 Batch:   0 Loss: 0.12057 Accuracy: 0.93750\n","Epoch:  47 Batch:   0 Loss: 0.13014 Accuracy: 0.93750\n","Epoch:  48 Batch:   0 Loss: 0.12128 Accuracy: 0.93750\n","Epoch:  49 Batch:   0 Loss: 0.11948 Accuracy: 0.96875\n","Test Loss: 0.17010 Accuracy: 0.95181\n","Epoch:  50 Batch:   0 Loss: 0.11771 Accuracy: 0.96875\n","Epoch:  51 Batch:   0 Loss: 0.11730 Accuracy: 0.96875\n","Epoch:  52 Batch:   0 Loss: 0.11589 Accuracy: 0.96875\n","Epoch:  53 Batch:   0 Loss: 0.11504 Accuracy: 0.96875\n","Epoch:  54 Batch:   0 Loss: 0.11428 Accuracy: 0.96875\n","Epoch:  55 Batch:   0 Loss: 0.11367 Accuracy: 0.96875\n","Epoch:  56 Batch:   0 Loss: 0.11309 Accuracy: 0.96875\n","Epoch:  57 Batch:   0 Loss: 0.11299 Accuracy: 0.96875\n","Epoch:  58 Batch:   0 Loss: 0.11298 Accuracy: 0.96875\n","Epoch:  59 Batch:   0 Loss: 0.11309 Accuracy: 0.96875\n","Test Loss: 0.16163 Accuracy: 0.95320\n","Epoch:  60 Batch:   0 Loss: 0.12952 Accuracy: 0.93750\n","Epoch:  61 Batch:   0 Loss: 0.11640 Accuracy: 0.96875\n","Epoch:  62 Batch:   0 Loss: 0.11453 Accuracy: 0.96875\n","Epoch:  63 Batch:   0 Loss: 0.11409 Accuracy: 0.96875\n","Epoch:  64 Batch:   0 Loss: 0.11397 Accuracy: 0.96875\n","Epoch:  65 Batch:   0 Loss: 0.11395 Accuracy: 0.96875\n","Epoch:  66 Batch:   0 Loss: 0.11395 Accuracy: 0.96875\n","Epoch:  67 Batch:   0 Loss: 0.11395 Accuracy: 0.96875\n","Epoch:  68 Batch:   0 Loss: 0.11395 Accuracy: 0.96875\n","Epoch:  69 Batch:   0 Loss: 0.11386 Accuracy: 0.96875\n","Test Loss: 0.17670 Accuracy: 0.94842\n","Epoch:  70 Batch:   0 Loss: 0.11378 Accuracy: 0.96875\n","Epoch:  71 Batch:   0 Loss: 0.11361 Accuracy: 0.96875\n","Epoch:  72 Batch:   0 Loss: 0.11363 Accuracy: 0.96875\n","Epoch:  73 Batch:   0 Loss: 0.11350 Accuracy: 0.96875\n","Epoch:  74 Batch:   0 Loss: 0.11848 Accuracy: 0.96875\n","Epoch:  75 Batch:   0 Loss: 0.11575 Accuracy: 0.96875\n","Epoch:  76 Batch:   0 Loss: 0.11500 Accuracy: 0.96875\n","Epoch:  77 Batch:   0 Loss: 0.11471 Accuracy: 0.96875\n","Epoch:  78 Batch:   0 Loss: 0.11461 Accuracy: 0.96875\n","Epoch:  79 Batch:   0 Loss: 0.11444 Accuracy: 0.96875\n","Test Loss: 0.17442 Accuracy: 0.95058\n","Epoch:  80 Batch:   0 Loss: 0.11364 Accuracy: 0.96875\n","Epoch:  81 Batch:   0 Loss: 0.11363 Accuracy: 0.96875\n","Epoch:  82 Batch:   0 Loss: 0.11289 Accuracy: 0.96875\n","Epoch:  83 Batch:   0 Loss: 0.11364 Accuracy: 0.96875\n","Epoch:  84 Batch:   0 Loss: 0.11388 Accuracy: 0.96875\n","Epoch:  85 Batch:   0 Loss: 0.11404 Accuracy: 0.96875\n","Epoch:  86 Batch:   0 Loss: 0.11419 Accuracy: 0.96875\n","Epoch:  87 Batch:   0 Loss: 0.11432 Accuracy: 0.96875\n","Epoch:  88 Batch:   0 Loss: 0.11445 Accuracy: 0.96875\n","Epoch:  89 Batch:   0 Loss: 0.11461 Accuracy: 0.96875\n","Test Loss: 0.17442 Accuracy: 0.95073\n","Epoch:  90 Batch:   0 Loss: 0.11473 Accuracy: 0.96875\n","Epoch:  91 Batch:   0 Loss: 0.11485 Accuracy: 0.96875\n","Epoch:  92 Batch:   0 Loss: 0.11494 Accuracy: 0.96875\n","Epoch:  93 Batch:   0 Loss: 0.11498 Accuracy: 0.96875\n","Epoch:  94 Batch:   0 Loss: 0.14838 Accuracy: 0.96875\n","Epoch:  95 Batch:   0 Loss: 0.11961 Accuracy: 0.93750\n","Epoch:  96 Batch:   0 Loss: 0.11585 Accuracy: 0.96875\n","Epoch:  97 Batch:   0 Loss: 0.11476 Accuracy: 0.96875\n","Epoch:  98 Batch:   0 Loss: 0.11441 Accuracy: 0.96875\n","Epoch:  99 Batch:   0 Loss: 0.11427 Accuracy: 0.96875\n","Test Loss: 0.16850 Accuracy: 0.95443\n","Epoch: 100 Batch:   0 Loss: 0.11428 Accuracy: 0.96875\n","Epoch: 101 Batch:   0 Loss: 0.11426 Accuracy: 0.96875\n","Epoch: 102 Batch:   0 Loss: 0.11429 Accuracy: 0.96875\n","Epoch: 103 Batch:   0 Loss: 0.11426 Accuracy: 0.96875\n","Epoch: 104 Batch:   0 Loss: 0.11431 Accuracy: 0.96875\n","Epoch: 105 Batch:   0 Loss: 0.11429 Accuracy: 0.96875\n","Epoch: 106 Batch:   0 Loss: 0.11434 Accuracy: 0.96875\n","Epoch: 107 Batch:   0 Loss: 0.11118 Accuracy: 0.96875\n","Epoch: 108 Batch:   0 Loss: 0.11214 Accuracy: 0.96875\n","Epoch: 109 Batch:   0 Loss: 0.11233 Accuracy: 0.96875\n","Test Loss: 0.17070 Accuracy: 0.95335\n","Epoch: 110 Batch:   0 Loss: 0.11238 Accuracy: 0.96875\n","Epoch: 111 Batch:   0 Loss: 0.11256 Accuracy: 0.96875\n","Epoch: 112 Batch:   0 Loss: 0.11277 Accuracy: 0.96875\n","Epoch: 113 Batch:   0 Loss: 0.11303 Accuracy: 0.96875\n","Epoch: 114 Batch:   0 Loss: 0.11327 Accuracy: 0.96875\n","Epoch: 115 Batch:   0 Loss: 0.11351 Accuracy: 0.96875\n","Epoch: 116 Batch:   0 Loss: 0.11373 Accuracy: 0.96875\n","Epoch: 117 Batch:   0 Loss: 0.11392 Accuracy: 0.96875\n","Epoch: 118 Batch:   0 Loss: 0.11410 Accuracy: 0.96875\n","Epoch: 119 Batch:   0 Loss: 0.11426 Accuracy: 0.96875\n","Test Loss: 0.17475 Accuracy: 0.95166\n","Epoch: 120 Batch:   0 Loss: 0.11440 Accuracy: 0.96875\n","Epoch: 121 Batch:   0 Loss: 0.11452 Accuracy: 0.96875\n","Epoch: 122 Batch:   0 Loss: 0.11462 Accuracy: 0.96875\n","Epoch: 123 Batch:   0 Loss: 0.11358 Accuracy: 0.96875\n","Epoch: 124 Batch:   0 Loss: 0.11436 Accuracy: 0.96875\n","Epoch: 125 Batch:   0 Loss: 0.11464 Accuracy: 0.96875\n","Epoch: 126 Batch:   0 Loss: 0.11478 Accuracy: 0.96875\n","Epoch: 127 Batch:   0 Loss: 0.11488 Accuracy: 0.96875\n","Epoch: 128 Batch:   0 Loss: 0.11495 Accuracy: 0.96875\n","Epoch: 129 Batch:   0 Loss: 0.11273 Accuracy: 0.96875\n","Test Loss: 0.17404 Accuracy: 0.95243\n","Epoch: 130 Batch:   0 Loss: 0.11396 Accuracy: 0.96875\n","Epoch: 131 Batch:   0 Loss: 0.11427 Accuracy: 0.96875\n","Epoch: 132 Batch:   0 Loss: 0.11445 Accuracy: 0.96875\n","Epoch: 133 Batch:   0 Loss: 0.11462 Accuracy: 0.96875\n","Epoch: 134 Batch:   0 Loss: 0.11479 Accuracy: 0.96875\n","Epoch: 135 Batch:   0 Loss: 0.11496 Accuracy: 0.96875\n","Epoch: 136 Batch:   0 Loss: 0.11584 Accuracy: 0.96875\n","Epoch: 137 Batch:   0 Loss: 0.11540 Accuracy: 0.96875\n","Epoch: 138 Batch:   0 Loss: 0.11554 Accuracy: 0.96875\n","Epoch: 139 Batch:   0 Loss: 0.11568 Accuracy: 0.96875\n","Test Loss: 0.17603 Accuracy: 0.95058\n","Epoch: 140 Batch:   0 Loss: 0.11623 Accuracy: 0.96875\n","Epoch: 141 Batch:   0 Loss: 0.11643 Accuracy: 0.96875\n","Epoch: 142 Batch:   0 Loss: 0.11666 Accuracy: 0.96875\n","Epoch: 143 Batch:   0 Loss: 0.11691 Accuracy: 0.96875\n","Epoch: 144 Batch:   0 Loss: 0.11713 Accuracy: 0.96875\n","Epoch: 145 Batch:   0 Loss: 0.11734 Accuracy: 0.96875\n","Epoch: 146 Batch:   0 Loss: 0.11753 Accuracy: 0.96875\n","Epoch: 147 Batch:   0 Loss: 0.11771 Accuracy: 0.96875\n","Epoch: 148 Batch:   0 Loss: 0.11788 Accuracy: 0.96875\n","Epoch: 149 Batch:   0 Loss: 0.11803 Accuracy: 0.96875\n","Test Loss: 0.17663 Accuracy: 0.95043\n","Epoch: 150 Batch:   0 Loss: 0.11818 Accuracy: 0.96875\n","Epoch: 151 Batch:   0 Loss: 0.11836 Accuracy: 0.96875\n","Epoch: 152 Batch:   0 Loss: 0.11841 Accuracy: 0.96875\n","Epoch: 153 Batch:   0 Loss: 0.11851 Accuracy: 0.96875\n","Epoch: 154 Batch:   0 Loss: 0.11860 Accuracy: 0.96875\n","Epoch: 155 Batch:   0 Loss: 0.11870 Accuracy: 0.96875\n","Epoch: 156 Batch:   0 Loss: 0.11880 Accuracy: 0.96875\n","Epoch: 157 Batch:   0 Loss: 0.11889 Accuracy: 0.96875\n","Epoch: 158 Batch:   0 Loss: 0.11897 Accuracy: 0.96875\n","Epoch: 159 Batch:   0 Loss: 0.11906 Accuracy: 0.96875\n","Test Loss: 0.17698 Accuracy: 0.94996\n","Epoch: 160 Batch:   0 Loss: 0.11913 Accuracy: 0.96875\n","Epoch: 161 Batch:   0 Loss: 0.11921 Accuracy: 0.96875\n","Epoch: 162 Batch:   0 Loss: 0.11928 Accuracy: 0.96875\n","Epoch: 163 Batch:   0 Loss: 0.11933 Accuracy: 0.96875\n","Epoch: 164 Batch:   0 Loss: 0.11940 Accuracy: 0.96875\n","Epoch: 165 Batch:   0 Loss: 0.11947 Accuracy: 0.96875\n","Epoch: 166 Batch:   0 Loss: 0.11953 Accuracy: 0.96875\n","Epoch: 167 Batch:   0 Loss: 0.11676 Accuracy: 0.96875\n","Epoch: 168 Batch:   0 Loss: 0.11796 Accuracy: 0.96875\n","Epoch: 169 Batch:   0 Loss: 0.11835 Accuracy: 0.96875\n","Test Loss: 0.17508 Accuracy: 0.95089\n","Epoch: 170 Batch:   0 Loss: 0.11840 Accuracy: 0.96875\n","Epoch: 171 Batch:   0 Loss: 0.12104 Accuracy: 0.96875\n","Epoch: 172 Batch:   0 Loss: 0.12016 Accuracy: 0.96875\n","Epoch: 173 Batch:   0 Loss: 0.12111 Accuracy: 0.96875\n","Epoch: 174 Batch:   0 Loss: 0.12081 Accuracy: 0.96875\n","Epoch: 175 Batch:   0 Loss: 0.12202 Accuracy: 0.96875\n","Epoch: 176 Batch:   0 Loss: 0.12133 Accuracy: 0.96875\n","Epoch: 177 Batch:   0 Loss: 0.12281 Accuracy: 0.96875\n","Epoch: 178 Batch:   0 Loss: 0.12138 Accuracy: 0.96875\n","Epoch: 179 Batch:   0 Loss: 0.12364 Accuracy: 0.96875\n","Test Loss: 0.17633 Accuracy: 0.95058\n","Epoch: 180 Batch:   0 Loss: 0.12145 Accuracy: 0.96875\n","Epoch: 181 Batch:   0 Loss: 0.12393 Accuracy: 0.96875\n","Epoch: 182 Batch:   0 Loss: 0.12176 Accuracy: 0.96875\n","Epoch: 183 Batch:   0 Loss: 0.12381 Accuracy: 0.96875\n","Epoch: 184 Batch:   0 Loss: 0.12116 Accuracy: 0.96875\n","Epoch: 185 Batch:   0 Loss: 0.12418 Accuracy: 0.96875\n","Epoch: 186 Batch:   0 Loss: 0.12152 Accuracy: 0.96875\n","Epoch: 187 Batch:   0 Loss: 0.12421 Accuracy: 0.96875\n","Epoch: 188 Batch:   0 Loss: 0.12399 Accuracy: 0.96875\n","Epoch: 189 Batch:   0 Loss: 0.12391 Accuracy: 0.96875\n","Test Loss: 0.17731 Accuracy: 0.94950\n","Epoch: 190 Batch:   0 Loss: 0.12320 Accuracy: 0.96875\n","Epoch: 191 Batch:   0 Loss: 0.12386 Accuracy: 0.96875\n","Epoch: 192 Batch:   0 Loss: 0.12362 Accuracy: 0.96875\n","Epoch: 193 Batch:   0 Loss: 0.12439 Accuracy: 0.96875\n","Epoch: 194 Batch:   0 Loss: 0.12314 Accuracy: 0.96875\n","Epoch: 195 Batch:   0 Loss: 0.12475 Accuracy: 0.96875\n","Epoch: 196 Batch:   0 Loss: 0.12355 Accuracy: 0.96875\n","Epoch: 197 Batch:   0 Loss: 0.12505 Accuracy: 0.96875\n","Epoch: 198 Batch:   0 Loss: 0.12378 Accuracy: 0.96875\n","Epoch: 199 Batch:   0 Loss: 0.12568 Accuracy: 0.96875\n","Test Loss: 0.17748 Accuracy: 0.95027\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-uypXmz1QJ2h","colab_type":"text"},"source":["# Model Evaluation"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-08-15T13:07:22.671507Z","start_time":"2018-08-15T13:07:22.645050Z"},"id":"VYxefAD3QJ2j","colab_type":"code","outputId":"f8b711a1-bae4-4bf5-9a62-935838844601","executionInfo":{"status":"ok","timestamp":1567154584845,"user_tz":-330,"elapsed":96103,"user":{"displayName":"Gokul Hari","photoUrl":"","userId":"16159457985484250305"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n","pred = sess.run(protoNN.predictions, feed_dict={X: x_test, Y: y_test})\n","# W, B, Z are tensorflow graph nodes\n","W, B, Z, _ = protoNN.getModelMatrices()\n","matrixList = sess.run([W, B, Z])\n","sparcityList = [SPAR_W, SPAR_B, SPAR_Z]\n","nnz, size, sparse = getModelSize(matrixList, sparcityList)\n","print(\"Final test accuracy\", acc)\n","print(\"Model size constraint (Bytes): \", size)\n","print(\"Number of non-zeros: \", nnz)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Final test accuracy 0.9502618\n","Model size constraint (Bytes):  2020\n","Number of non-zeros:  505\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yJM9puhcQJ2t","colab_type":"code","outputId":"d8e743e0-1068-4681-e3bc-3b650ab66a3c","executionInfo":{"status":"ok","timestamp":1567154584848,"user_tz":-330,"elapsed":91542,"user":{"displayName":"Gokul Hari","photoUrl":"","userId":"16159457985484250305"}},"colab":{"base_uri":"https://localhost:8080/","height":217}},"source":["from sklearn.metrics import confusion_matrix,classification_report\n","y_test = np.argmax(y_test,axis=1)\n","print (confusion_matrix(y_test,pred))\n","print (classification_report(y_test,pred,digits=5))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[2948  299]\n"," [  24 3223]]\n","              precision    recall  f1-score   support\n","\n","           0    0.99192   0.90791   0.94806      3247\n","           1    0.91511   0.99261   0.95228      3247\n","\n","    accuracy                        0.95026      6494\n","   macro avg    0.95351   0.95026   0.95017      6494\n","weighted avg    0.95351   0.95026   0.95017      6494\n","\n"],"name":"stdout"}]}]}