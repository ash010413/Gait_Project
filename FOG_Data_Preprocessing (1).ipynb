{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad26af23",
   "metadata": {},
   "source": [
    "1 3 5 8 9 \\n\n",
    "2 3 6 8 9 \\n\n",
    "3 5 6 7 9 \\n\n",
    "3 5 7 8 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "839370a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import os\n",
    "import joblib\n",
    "import gc\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras import layers\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.integrate import simps\n",
    "from numpy.fft import fft\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.signal import butter, lfilter, welch\n",
    "from joblib import parallel_backend\n",
    "import dill\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sb.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "162ba02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_prefog(dataset,window_length = 1):\n",
    "  dataset.drop(index = list(dataset[dataset['Action'] == 0].index),inplace=True)\n",
    "  window_length = 64*window_length\n",
    "\n",
    "  fog_index=[]\n",
    "  for i in dataset.index:\n",
    "      if dataset.loc[i,'Action'] == 2:\n",
    "        fog_index.append(i)\n",
    "  fog_index\n",
    "\n",
    "  start_indices=[]\n",
    "  for i in fog_index:\n",
    "    if (dataset.loc[i-1,'Action']!=dataset.loc[i,'Action']):\n",
    "      start_indices.append(i)\n",
    "\n",
    "\n",
    "  prefog=[]\n",
    "  for start in start_indices:\n",
    "    prefog_start = [x for x in range(start-window_length,start)]\n",
    "    prefog.append(prefog_start)\n",
    "\n",
    "  prefog = [item for sublist in prefog for item in sublist]\n",
    "\n",
    "  for i in prefog:\n",
    "       dataset.loc[i,'Action'] = 3\n",
    "  dataset['Action'] = dataset['Action'] - 1\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fef8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e02d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_combo = \"1_6_7_8_9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b620d42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>A_F</th>\n",
       "      <th>A_V</th>\n",
       "      <th>A_L</th>\n",
       "      <th>L_F</th>\n",
       "      <th>L_V</th>\n",
       "      <th>L_L</th>\n",
       "      <th>T_F</th>\n",
       "      <th>T_V</th>\n",
       "      <th>T_L</th>\n",
       "      <th>Action</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>670000</td>\n",
       "      <td>-151</td>\n",
       "      <td>990</td>\n",
       "      <td>267</td>\n",
       "      <td>63</td>\n",
       "      <td>990</td>\n",
       "      <td>80</td>\n",
       "      <td>-9</td>\n",
       "      <td>1019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>670015</td>\n",
       "      <td>-151</td>\n",
       "      <td>1000</td>\n",
       "      <td>277</td>\n",
       "      <td>63</td>\n",
       "      <td>981</td>\n",
       "      <td>80</td>\n",
       "      <td>-9</td>\n",
       "      <td>1009</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>S02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>670031</td>\n",
       "      <td>-181</td>\n",
       "      <td>1009</td>\n",
       "      <td>247</td>\n",
       "      <td>63</td>\n",
       "      <td>981</td>\n",
       "      <td>70</td>\n",
       "      <td>-9</td>\n",
       "      <td>1019</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>S02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>670046</td>\n",
       "      <td>-161</td>\n",
       "      <td>990</td>\n",
       "      <td>237</td>\n",
       "      <td>81</td>\n",
       "      <td>981</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1019</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>S02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>670062</td>\n",
       "      <td>-151</td>\n",
       "      <td>1000</td>\n",
       "      <td>257</td>\n",
       "      <td>81</td>\n",
       "      <td>981</td>\n",
       "      <td>80</td>\n",
       "      <td>-9</td>\n",
       "      <td>1000</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>S02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327399</th>\n",
       "      <td>1469937</td>\n",
       "      <td>343</td>\n",
       "      <td>1009</td>\n",
       "      <td>118</td>\n",
       "      <td>818</td>\n",
       "      <td>481</td>\n",
       "      <td>-50</td>\n",
       "      <td>864</td>\n",
       "      <td>485</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>S05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327400</th>\n",
       "      <td>1469953</td>\n",
       "      <td>313</td>\n",
       "      <td>990</td>\n",
       "      <td>128</td>\n",
       "      <td>818</td>\n",
       "      <td>527</td>\n",
       "      <td>-80</td>\n",
       "      <td>844</td>\n",
       "      <td>476</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>S05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327401</th>\n",
       "      <td>1469968</td>\n",
       "      <td>333</td>\n",
       "      <td>1000</td>\n",
       "      <td>108</td>\n",
       "      <td>827</td>\n",
       "      <td>500</td>\n",
       "      <td>-20</td>\n",
       "      <td>864</td>\n",
       "      <td>466</td>\n",
       "      <td>194</td>\n",
       "      <td>0</td>\n",
       "      <td>S05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327402</th>\n",
       "      <td>1469984</td>\n",
       "      <td>323</td>\n",
       "      <td>1039</td>\n",
       "      <td>108</td>\n",
       "      <td>836</td>\n",
       "      <td>500</td>\n",
       "      <td>-40</td>\n",
       "      <td>854</td>\n",
       "      <td>457</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>S05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327403</th>\n",
       "      <td>1470000</td>\n",
       "      <td>333</td>\n",
       "      <td>980</td>\n",
       "      <td>168</td>\n",
       "      <td>836</td>\n",
       "      <td>500</td>\n",
       "      <td>-60</td>\n",
       "      <td>873</td>\n",
       "      <td>438</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>S05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1327404 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            time  A_F   A_V  A_L  L_F  L_V  L_L  T_F   T_V  T_L  Action name\n",
       "0         670000 -151   990  267   63  990   80   -9  1019    0       0  S02\n",
       "1         670015 -151  1000  277   63  981   80   -9  1009   58       0  S02\n",
       "2         670031 -181  1009  247   63  981   70   -9  1019   38       0  S02\n",
       "3         670046 -161   990  237   81  981   80    0  1019   48       0  S02\n",
       "4         670062 -151  1000  257   81  981   80   -9  1000   38       0  S02\n",
       "...          ...  ...   ...  ...  ...  ...  ...  ...   ...  ...     ...  ...\n",
       "1327399  1469937  343  1009  118  818  481  -50  864   485  203       0  S05\n",
       "1327400  1469953  313   990  128  818  527  -80  844   476  223       0  S05\n",
       "1327401  1469968  333  1000  108  827  500  -20  864   466  194       0  S05\n",
       "1327402  1469984  323  1039  108  836  500  -40  854   457  233       0  S05\n",
       "1327403  1470000  333   980  168  836  500  -60  873   438  223       0  S05\n",
       "\n",
       "[1327404 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path=r\"Data_Train\"\n",
    "path=f\"{patient_combo}/Labelled_Files_Train\"\n",
    "\n",
    "people = []\n",
    "for person in os.listdir(data_path):\n",
    "    if '.txt' in person:\n",
    "        people.append(person)\n",
    "for window_length in range(1,5):\n",
    "    for person in people:\n",
    "        name = person.split('R')[0]\n",
    "        print (name)\n",
    "        file = data_path+\"/\"+person\n",
    "        temp = pd.read_csv(file,delimiter= \" \", header = None)\n",
    "        print (person,' is read',end = '\\t')\n",
    "        if 2 in temp[max(temp.columns)].unique():\n",
    "            print ('Adding {} to dataset'.format(person),end = '\\t')\n",
    "            temp.columns = ['time','A_F','A_V','A_L','L_F','L_V','L_L','T_F','T_V','T_L','Action']\n",
    "            temp = label_prefog(temp,window_length).reset_index(drop=True)\n",
    "            temp['name'] = name\n",
    "            print ('{} is labelled'.format(person))\n",
    "            dataset = pd.concat([dataset,temp],axis = 0)\n",
    "\n",
    "        print ('')\n",
    "    dataset.reset_index(drop =True,inplace=True)\n",
    "    to_path = path + \"/raw_labelled\"\n",
    "    to_name = to_path +\"/win_\"+str(window_length)+\".csv\"\n",
    "\n",
    "    dataset.to_csv(to_name,index = False)\n",
    "\n",
    "\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dd99ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S05    535064\n",
       "S03    430092\n",
       "S02    362248\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16e6ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(act,window_length,dataframe):\n",
    "\n",
    "  indices = list(dataframe[dataframe.Action == act].index)\n",
    "  groups = []\n",
    "  temp = []\n",
    "  group_count = 0\n",
    "  for i in range(len(indices)):\n",
    "    if i == len(indices)-1:\n",
    "      temp.append(indices[i])\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "      break\n",
    "    temp.append(indices[i])\n",
    "    if indices[i]+1 != indices[i+1]:\n",
    "      group_count+=1\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "\n",
    "  fs = 64\n",
    "  window_length = 1\n",
    "  # window_length = window_length*fs\n",
    "\n",
    "  final_dataframe = pd.DataFrame()\n",
    "  for i in groups:\n",
    "    required = math.floor(len(i)/(window_length*fs))\n",
    "\n",
    "    req_index = i[0:(required*fs)]\n",
    "\n",
    "    final_dataframe = pd.concat([final_dataframe,dataframe.iloc[req_index,:]],axis = 0)\n",
    "  return final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85fd9a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for window_length in range(1,5):\n",
    "\n",
    "  # path = os.getcwd()+\"/dataset_fog_release/dataset\"\n",
    "  name = path+\"/raw_labelled/win_\"+str(window_length)+\".csv\"\n",
    "  dataframe = pd.read_csv(name)\n",
    "\n",
    "  activities = []\n",
    "  for act in range(3):\n",
    "    activities.append(create_window(act,window_length,dataframe))\n",
    "  to_write = pd.concat(activities,axis = 0)\n",
    "  to_path = path + \"/windows\"+\"/windowed_\"+str(window_length)+\".csv\"\n",
    "  to_write.to_csv(to_path,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65d8e702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_F A_V A_L L_F L_V L_L T_F T_V T_L A_F A_V A_L L_F L_V L_L T_F T_V T_L A_F A_V A_L L_F L_V L_L T_F T_V T_L A_F A_V A_L L_F L_V L_L T_F T_V T_L "
     ]
    }
   ],
   "source": [
    "fs = 64\n",
    "for window_length in range(1,5):\n",
    "  w = window_length*fs\n",
    "  FE_path = path + \"/windows/windowed_\"\n",
    "  name = FE_path + str(window_length) + \".csv\"\n",
    "  dataframe = pd.read_csv(name)\n",
    "\n",
    "  df = dataframe.drop(columns=['time','Action','name'])\n",
    "    \n",
    "  acc_readings = ['A_F', 'A_V', 'A_L', 'L_F', 'L_V', 'L_L', 'T_F', 'T_V', 'T_L']\n",
    "  df_acc = df[acc_readings]\n",
    "\n",
    "  stat = pd.DataFrame()\n",
    "\n",
    "  col= list(df.columns)\n",
    "  for s in col:\n",
    "    print (s,end=\" \")\n",
    "    mn =[]\n",
    "    mx =[]\n",
    "    mi =[]\n",
    "    var = []\n",
    "    std = []\n",
    "    mav = []\n",
    "    rms =[]\n",
    "    zcr =[]\n",
    "    skewness = []\n",
    "    sma = []\n",
    "    wl = []\n",
    "    p2p = []     # Peak-to-peak amplitude\n",
    "    cf = []      # Crest factor\n",
    "    sf = []      # Shape factor\n",
    "    ifac = []    # Impulse factor\n",
    "    for i in range(0,len(df),w):\n",
    "        window_data = df[s].iloc[i:i+w]\n",
    "        mn_  = np.mean(window_data)\n",
    "        mx_  = np.max(window_data)\n",
    "        mi_  = np.min(window_data)\n",
    "        var_  = np.var(window_data)\n",
    "        std_  = np.std(window_data)\n",
    "        mav_  = np.mean(abs(window_data))\n",
    "        rms_  = np.sqrt(np.mean((window_data)**2))\n",
    "        zcr_ = ((window_data > 0).astype(int)).diff().abs().sum() / 2.0\n",
    "        p2p_ = mx_ - mi_\n",
    "        cf_ = mx_ / rms_ if rms_ != 0 else np.nan\n",
    "        sf_ = rms_ / mav_ if mav_ != 0 else np.nan\n",
    "        ifac_ = mx_ / mav_ if mav_ != 0 else np.nan\n",
    "        skewness_ = skew(window_data)\n",
    "        sma_ = np.sum(np.abs(window_data))\n",
    "        wl_ = np.sum(np.abs(np.diff(window_data)))\n",
    "        \n",
    "        mn.append(mn_)\n",
    "        mx.append(mx_)\n",
    "        mi.append(mi_)\n",
    "        var.append(var_)\n",
    "        std.append(std_)\n",
    "        mav.append(mav_)\n",
    "        rms.append(rms_)\n",
    "        zcr.append(zcr_)\n",
    "        p2p.append(p2p_)         # Append peak-to-peak\n",
    "        cf.append(cf_)           # Append crest factor\n",
    "        sf.append(sf_)           # Append shape factor\n",
    "        ifac.append(ifac_)       # Append impulse factor\n",
    "        skewness.append(skewness_)\n",
    "        sma.append(sma_)\n",
    "        wl.append(wl_)\n",
    "        \n",
    "    stat['mean_'+s] = mn\n",
    "    stat['max_'+s] = mx\n",
    "    stat['min_'+s] = mi\n",
    "    stat['var_'+s] = var\n",
    "    stat['std_'+s] = std\n",
    "    stat['rms_'+s] = rms\n",
    "    stat['mav_'+s] = mav\n",
    "    stat['zcr_'+s] = zcr\n",
    "    stat['p2p_'+s] = p2p\n",
    "    stat['cf_'+s] = cf\n",
    "    stat['sf_'+s] = sf\n",
    "    stat['ifac_'+s] = ifac\n",
    "    stat['skew_'+s] = skewness\n",
    "    stat['sma_'+s] = sma\n",
    "    stat['wl_'+s] = wl\n",
    "\n",
    "  # Create empty dataframes for new features\n",
    "  cross_correlation_df = pd.DataFrame()\n",
    "  covariance_df = pd.DataFrame()\n",
    "\n",
    "  # Calculate cross-correlation and covariance for each window\n",
    "  for i in range(0,len(df_acc),w):\n",
    "      window_data = df_acc.iloc[i:i+w]\n",
    "        \n",
    "      # calculate cross-correlation\n",
    "      for col1 in df_acc.columns:\n",
    "          for col2 in df_acc.columns:\n",
    "              if col1 != col2:  # avoid calculating correlation of a column with itself\n",
    "                  corr = np.correlate(window_data[col1], window_data[col2])[0]  \n",
    "                  cross_correlation_df.loc[i//w, 'corr_'+col1+'_'+col2] = corr\n",
    "                    \n",
    "      # calculate covariance\n",
    "      cov_matrix = np.cov(window_data, rowvar=False)\n",
    "      cov_df = pd.DataFrame(cov_matrix, index=acc_readings, columns=acc_readings)\n",
    "        \n",
    "      # flatten the covariance matrix and add to dataframe\n",
    "      for col1 in cov_df.columns:\n",
    "          for col2 in cov_df.columns:\n",
    "              if col1 != col2:  # avoid calculating covariance of a column with itself\n",
    "                  covariance_df.loc[i//w, 'cov_'+col1+'_'+col2] = cov_df.loc[col1, col2]\n",
    "\n",
    "  # Concatenate the cross-correlation and covariance features with the existing dataframe\n",
    "  stat = pd.concat([stat, cross_correlation_df, covariance_df], axis=1)\n",
    "    \n",
    "  # Add jerk columns for each axis\n",
    "  for axis in [\"A_F\", \"A_V\", \"A_L\", \"L_F\", \"L_V\", \"L_L\", \"T_F\", \"T_V\", \"T_L\"]:\n",
    "    stat[axis+\"_jerk\"] = df[axis].diff()\n",
    "\n",
    "  df['ankle_vector'] = df[['A_F', 'A_V', 'A_L']].values.tolist()\n",
    "  df['trunk_vector'] = df[['T_F', 'T_V', 'T_L']].values.tolist()\n",
    "  df['upper_leg_vector'] = df[['L_F', 'L_V', 'L_L']].values.tolist()\n",
    "\n",
    "  df['ankle_vector'] = df['ankle_vector'].apply(np.array)\n",
    "  df['trunk_vector'] = df['trunk_vector'].apply(np.array)\n",
    "  df['upper_leg_vector'] = df['upper_leg_vector'].apply(np.array)\n",
    "    \n",
    "  stat['angle_trunk_ankle'] = df.apply(lambda row: np.arccos(np.dot(row['trunk_vector'], row['ankle_vector']) / \n",
    "                                     (np.linalg.norm(row['trunk_vector']) * np.linalg.norm(row['ankle_vector']))), axis=1)\n",
    "\n",
    "  stat['angle_trunk_upper_leg'] = df.apply(lambda row: np.arccos(np.dot(row['trunk_vector'], row['upper_leg_vector']) / \n",
    "                                         (np.linalg.norm(row['trunk_vector']) * np.linalg.norm(row['upper_leg_vector']))), axis=1)\n",
    "\n",
    "  stat['angle_upper_leg_ankle'] = df.apply(lambda row: np.arccos(np.dot(row['upper_leg_vector'], row['ankle_vector']) / \n",
    "                                         (np.linalg.norm(row['upper_leg_vector']) * np.linalg.norm(row['ankle_vector']))), axis=1)\n",
    "    \n",
    "  rel_acc_trunk_ankle= df['trunk_vector'] - df['ankle_vector']\n",
    "  rel_acc_trunk_upper_leg= df['trunk_vector'] - df['upper_leg_vector']\n",
    "  rel_acc_upper_leg_ankle= df['upper_leg_vector'] - df['ankle_vector']\n",
    "    \n",
    "  stat['rel_acc_trunk_ankle_mag'] = rel_acc_trunk_ankle.apply(np.linalg.norm)\n",
    "  stat['rel_acc_trunk_upper_leg_mag'] = rel_acc_trunk_upper_leg.apply(np.linalg.norm)\n",
    "  stat['rel_acc_upper_leg_ankle_mag'] = rel_acc_upper_leg_ankle.apply(np.linalg.norm)\n",
    "\n",
    "  stat['rel_acc_trunk_ankle_x'] = rel_acc_trunk_ankle.apply(lambda v: v[0])\n",
    "  stat['rel_acc_trunk_ankle_y'] = rel_acc_trunk_ankle.apply(lambda v: v[1])\n",
    "  stat['rel_acc_trunk_ankle_z'] = rel_acc_trunk_ankle.apply(lambda v: v[2])\n",
    "  stat['rel_acc_trunk_upper_leg_x'] = rel_acc_trunk_upper_leg.apply(lambda v: v[0])\n",
    "  stat['rel_acc_trunk_upper_leg_y'] = rel_acc_trunk_upper_leg.apply(lambda v: v[1])\n",
    "  stat['rel_acc_trunk_upper_leg_z'] = rel_acc_trunk_upper_leg.apply(lambda v: v[2])\n",
    "  stat['rel_acc_upper_leg_ankle_x'] = rel_acc_upper_leg_ankle.apply(lambda v: v[0])\n",
    "  stat['rel_acc_upper_leg_ankle_y'] = rel_acc_upper_leg_ankle.apply(lambda v: v[1])\n",
    "  stat['rel_acc_upper_leg_ankle_z'] = rel_acc_upper_leg_ankle.apply(lambda v: v[2])\n",
    "\n",
    "  stat['ankle_magnitude'] = df['ankle_vector'].apply(np.linalg.norm)\n",
    "  stat['trunk_magnitude'] = df['trunk_vector'].apply(np.linalg.norm)\n",
    "  stat['upper_leg_magnitude'] = df['upper_leg_vector'].apply(np.linalg.norm)\n",
    "\n",
    "  df=df.drop(columns=['ankle_vector', 'trunk_vector', 'upper_leg_vector'])\n",
    "\n",
    "  import copy\n",
    "  stat1 = copy.copy(stat)\n",
    "  stat1['w'] = dataframe['Action'].iloc[[x for x in range(0,len(dataframe),w)]].to_list()\n",
    "  order = ['w']\n",
    "  order += stat1.columns.to_list()[:-1]\n",
    "  stat1 = stat1[order]\n",
    "  stat1.columns\n",
    "  col = stat1.columns.to_list()\n",
    "  col[0] = 0\n",
    "  stat1.columns = col\n",
    "  feature_name = path + \"/features/time_\"+str(window_length)+\".csv\"\n",
    "  stat1.to_csv(feature_name, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b72fa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze, power, dominant frequency, PSD and other features done\n",
      "Freeze, power, dominant frequency, PSD and other features done\n",
      "Freeze, power, dominant frequency, PSD and other features done\n",
      "Freeze, power, dominant frequency, PSD and other features done\n"
     ]
    }
   ],
   "source": [
    "#window_length = 3\n",
    "fs = 64\n",
    "for window_length in range(1,5):\n",
    "  w = window_length*fs\n",
    "  FE_path = path + \"/windows/windowed_\"\n",
    "  name = FE_path + str(window_length) + \".csv\"\n",
    "  dataframe = pd.read_csv(name)\n",
    "\n",
    "  df = dataframe.drop(columns=['time','Action','name'])\n",
    "\n",
    "  col= list(df.columns)\n",
    "\n",
    "  order=5\n",
    "\n",
    "  fi=pd.DataFrame()\n",
    "\n",
    "  power = pd.DataFrame()\n",
    "  bands = {'locomotor' :(0.5,3),'freeze' :(3,8)}\n",
    "  \n",
    "  # For dominant frequency and PSD\n",
    "  dom_freq_df = pd.DataFrame()\n",
    "  PSD_df = pd.DataFrame()\n",
    "\n",
    "  for s in col:\n",
    "      xtemp = []\n",
    "      xtemp1 = []\n",
    "      \n",
    "      dom_freq_temp = []  # for dominant frequency\n",
    "      PSD_temp = []  # for PSD\n",
    "\n",
    "      spectral_centroid_temp = []  # for spectral centroid\n",
    "      spectral_spread_temp = []  # for spectral spread\n",
    "      spectral_skewness_temp = []  # for spectral skewness\n",
    "      spectral_kurtosis_temp = []  # for spectral kurtosis\n",
    "      spectral_slope_temp = []  # for spectral slope\n",
    "      spectral_flux_temp = []  # for spectral flux\n",
    "      bandpower_temp = []  # for band power\n",
    "\n",
    "      \n",
    "      for i in range(0,len(df),w):\n",
    "          nyq=0.5*fs\n",
    "\n",
    "          #locomotor band 0.5-3hz\n",
    "          loc_low= 0.5/nyq\n",
    "          loc_high=3/nyq\n",
    "\n",
    "          #clipping off band from the window\n",
    "          b, a = butter(order, [loc_low, loc_high], btype='band')\n",
    "          y=lfilter(b,a,df[s].iloc[i:i+w])\n",
    "\n",
    "          #total power in locomotor band\n",
    "          e1=sum([x**2 for x in y])\n",
    "\n",
    "          #freeze band 3-8hz\n",
    "          frez_low= 3/nyq\n",
    "          frez_high=8/nyq\n",
    "\n",
    "          #clipping off band from the window\n",
    "          b1, a1 = butter(order, [frez_low, frez_high], btype='band')\n",
    "          y1=lfilter(b1,a1,df[s].iloc[i:i+w])\n",
    "          #total power in locomotor band\n",
    "          e2=sum([x**2 for x in y1])\n",
    "\n",
    "          FI=e2/e1\n",
    "          POW=e2+e1\n",
    "          xtemp.append(FI)\n",
    "          xtemp1.append(POW)\n",
    "          \n",
    "          # Compute PSD and dominant frequency\n",
    "          freq, psd = welch(df[s].iloc[i:i+w], fs)\n",
    "          dom_freq = freq[np.argmax(psd)]  # dominant frequency\n",
    "          total_psd = np.sum(psd)  # total power spectral density\n",
    "          \n",
    "          dom_freq_temp.append(dom_freq)\n",
    "          PSD_temp.append(total_psd)\n",
    "            \n",
    "          # Compute Spectral Centroid\n",
    "          spectral_centroid = np.sum(freq * psd) / np.sum(psd)\n",
    "          spectral_centroid_temp.append(spectral_centroid)\n",
    "\n",
    "          # Compute Spectral Spread\n",
    "          spectral_spread = np.sqrt(np.sum((freq - spectral_centroid)**2 * psd) / np.sum(psd))\n",
    "          spectral_spread_temp.append(spectral_spread)\n",
    "\n",
    "          # Compute Spectral Skewness\n",
    "          spectral_skewness = np.sum(((freq - spectral_centroid) / spectral_spread)**3 * psd) / np.sum(psd)\n",
    "          spectral_skewness_temp.append(spectral_skewness)\n",
    "\n",
    "          # Compute Spectral Kurtosis\n",
    "          spectral_kurtosis = np.sum(((freq - spectral_centroid) / spectral_spread)**4 * psd) / np.sum(psd) - 3\n",
    "          spectral_kurtosis_temp.append(spectral_kurtosis)\n",
    "\n",
    "          # Compute Spectral Slope\n",
    "          spectral_slope = np.polyfit(freq, psd, 1)[0]\n",
    "          spectral_slope_temp.append(spectral_slope)\n",
    "\n",
    "          # Compute Bandpower\n",
    "          bandpower = simps(psd, freq)\n",
    "          bandpower_temp.append(bandpower)\n",
    "\n",
    "      fi['FI'+s] = xtemp\n",
    "      power['P'+s] = xtemp1\n",
    "      \n",
    "      dom_freq_df['dom_freq_'+s] = dom_freq_temp  # add to df\n",
    "      PSD_df['PSD_'+s] = PSD_temp  # add to df\n",
    "\n",
    "      dom_freq_df['spectral_centroid_'+s] = spectral_centroid_temp  # add to df\n",
    "      PSD_df['spectral_spread_'+s] = spectral_spread_temp  # add to df\n",
    "      dom_freq_df['spectral_skewness_'+s] = spectral_skewness_temp  # add to df\n",
    "      PSD_df['spectral_kurtosis_'+s] = spectral_kurtosis_temp  # add to df\n",
    "      dom_freq_df['spectral_slope_'+s] = spectral_slope_temp  # add to df\n",
    "      PSD_df['bandpower_'+s] = bandpower_temp  # add to df\n",
    "\n",
    "  print (\"Freeze, power, dominant frequency, PSD and other features done\")\n",
    "\n",
    "  w = window_length*fs\n",
    "  E=[]\n",
    "  for i in range(0,len(df),w):\n",
    "    energy = np.sum((df.iloc[i:i+w,:])**2)\n",
    "    E.append(energy)\n",
    "  E = pd.DataFrame(E)\n",
    "  E.columns = [\"EN_\" + x for x in df.columns]\n",
    "\n",
    "  #Entropy\n",
    "  from scipy.signal import periodogram\n",
    "\n",
    "  peak_f = pd.DataFrame()\n",
    "  PSE = pd.DataFrame()\n",
    "  for s in col:\n",
    "    peakF = []\n",
    "    pse = []\n",
    "    for i in range(0,len(df),w):\n",
    "        f,Pxx_den = periodogram(df[s].iloc[i:i+w],fs)\n",
    "        p_norm = Pxx_den/sum(Pxx_den)\n",
    "        p_norm = list(filter(lambda a: a != 0, p_norm))\n",
    "        pse.append(-(np.sum(p_norm*np.log(p_norm))))\n",
    "        peak = (fs/w)*max(Pxx_den)\n",
    "        peakF.append(peak)\n",
    "    PSE['ENt_'+s] = pse\n",
    "    peak_f['peak_'+s] = peakF\n",
    "  PSE.fillna(0,inplace = True)\n",
    "\n",
    "  # Concatenating all the dataframes to create a final dataframe with all the features\n",
    "  freq = pd.concat([fi,power,E,PSE,peak_f, dom_freq_df, PSD_df],axis = 1)\n",
    "\n",
    "  feature_name = path + \"/features/freq_\"+str(window_length)+\".csv\"\n",
    "  freq.to_csv(feature_name, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c578a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
